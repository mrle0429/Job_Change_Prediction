{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a81d8f",
   "metadata": {},
   "source": [
    "# Data Science Job Change Prediction\n",
    "- Author: Le Liu\n",
    "- Course: COMP3010J Machine Learning\n",
    "\n",
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "This project aims to predict whether a candidate is looking for a job change based on various demographic and professional features. Then infer the key factors influencing their decision.\n",
    "\n",
    "**Dataset:** `data-science-job-change.csv`\n",
    "\n",
    "**Problem Type:** Binary Classification\n",
    "\n",
    "**Target Variable:** `target` (1.0 = Looking for job change, 0.0 = Not looking for job change)\n",
    "\n",
    "\n",
    "\n",
    "*Project Structure*\n",
    "\n",
    "1. **Introduction** - Project overview and objectives\n",
    "2. **Load and Analyse Data** - Data loading and initial exploration\n",
    "3. **Data Cleaning** - Handle missing values and data quality issues\n",
    "4. **Data Visualisation** - Exploratory data analysis with plots\n",
    "5. **Attribute Selection** - Feature selection and engineering\n",
    "6. **Model Selection and Experiments** - Train and compare models\n",
    "7. **Final Model Training** - Train the best model\n",
    "8. **Further Analysis and Discussion** - Model interpretation\n",
    "9. **Discussion** - Conclusions and future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6ec4c",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('data-science-job-change.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405bd547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\"*60)\n",
    "numerical_cols = ['city_development_index', 'training_hours', 'target']\n",
    "df[numerical_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50b11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(3),\n",
    "    'Data_Type': df.dtypes,\n",
    "    'Unique_Values': [df[col].nunique() for col in df.columns]\n",
    "})\n",
    "\n",
    "print(\"Data Health Report:\")\n",
    "print(\"=\"*80)\n",
    "print(missing_data.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21f389",
   "metadata": {},
   "source": [
    "### 2.1 Target Variable Analysis (Class Balance Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bbb356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Target variable analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"TARGET VARIABLE ANALYSIS: 'target' (Job Change Intention)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "target_counts = df['target'].value_counts().sort_index()\n",
    "target_pct = df['target'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(\"-\" * 50)\n",
    "for val, count, pct in zip(target_counts.index, target_counts.values, target_pct.values):\n",
    "    label = \"Not Looking for Change\" if val == 0.0 else \"Looking for Change\"\n",
    "    print(f\"  Class {int(val)} ({label:25s}): {count:>6,} ({pct:>5.2f}%)\")\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "imbalance_ratio = target_counts.max() / target_counts.min()\n",
    "print(f\"\\nImbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "axes[0].bar(target_counts.index, target_counts.values, color=['#3498db', '#e74c3c'], alpha=0.7)\n",
    "axes[0].set_xlabel('Target Class', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Target Variable Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_xticklabels(['Not Looking\\nfor Change (0)', 'Looking for\\nChange (1)'])\n",
    "for i, (val, count) in enumerate(zip(target_counts.index, target_counts.values)):\n",
    "    axes[0].text(i, count, f'{count:,}\\n({target_pct.values[i]:.1f}%)', \n",
    "                 ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "axes[1].pie(target_counts.values, labels=['Not Looking (0)', 'Looking (1)'], \n",
    "           autopct='%1.1f%%', colors=colors, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Target Class Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8eed09",
   "metadata": {},
   "source": [
    "### 2.2 Numerical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fe23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Select numerical features\n",
    "numerical_features = ['city_development_index', 'training_hours']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NUMERICAL FEATURES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Detailed statistics for each numerical feature\n",
    "for col in numerical_features:\n",
    "    print(f\"\\nFeature: '{col}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    data = df[col].dropna()\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"   Count:    {len(data):>10,}\")\n",
    "    print(f\"   Mean:     {data.mean():>10.4f}\")\n",
    "    print(f\"   Median:   {data.median():>10.4f}\")\n",
    "    print(f\"   Std Dev:  {data.std():>10.4f}\")\n",
    "    print(f\"   Min:      {data.min():>10.4f}\")\n",
    "    print(f\"   Max:      {data.max():>10.4f}\")\n",
    "    print(f\"   Range:    {data.max() - data.min():>10.4f}\")\n",
    "    print(f\"   Value Range: [{data.min()}, {data.max()}]\")\n",
    "    \n",
    "    # Distribution shape\n",
    "    skewness = stats.skew(data)\n",
    "    kurtosis = stats.kurtosis(data)\n",
    "    print(f\"\\n   Skewness: {skewness:>10.4f}  \", end=\"\")\n",
    "    if abs(skewness) < 0.5:\n",
    "        print(\"(Nearly symmetric)\")\n",
    "    elif skewness > 0:\n",
    "        print(\"(Right-skewed / Positive skew)\")\n",
    "    else:\n",
    "        print(\"Left-skewed / Negative skew)\")\n",
    "    \n",
    "    print(f\"   Kurtosis: {kurtosis:>10.4f}  \", end=\"\")\n",
    "    if abs(kurtosis) < 1:\n",
    "        print(\"(Normal tail)\")\n",
    "    elif kurtosis > 0:\n",
    "        print(\"(Heavy-tailed / Outliers present)\")\n",
    "    else:\n",
    "        print(\"(Light-tailed)\")\n",
    "    \n",
    "    # Outlier detection (IQR method)\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    \n",
    "    print(f\"\\n   Outliers: {len(outliers):>10,} ({len(outliers)/len(data)*100:.2f}%)\")\n",
    "    print(f\"   IQR Range: [{lower_bound:.4f}, {upper_bound:.4f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Numerical Features Distribution Analysis', fontsize=16, fontweight='bold', y=1.00)\n",
    "\n",
    "for idx, col in enumerate(numerical_features):\n",
    "    data = df[col].dropna()\n",
    "    \n",
    "    # Histogram with KDE\n",
    "    axes[idx, 0].hist(data, bins=50, edgecolor='black', alpha=0.7, color='steelblue', density=True)\n",
    "    data.plot(kind='kde', ax=axes[idx, 0], color='red', linewidth=2)\n",
    "    axes[idx, 0].set_title(f'{col} - Distribution', fontweight='bold')\n",
    "    axes[idx, 0].set_xlabel(col)\n",
    "    axes[idx, 0].set_ylabel('Density')\n",
    "    axes[idx, 0].axvline(data.mean(), color='green', linestyle='--', linewidth=2, label=f'Mean: {data.mean():.2f}')\n",
    "    axes[idx, 0].axvline(data.median(), color='orange', linestyle='--', linewidth=2, label=f'Median: {data.median():.2f}')\n",
    "    axes[idx, 0].legend()\n",
    "    \n",
    "    # Box plot\n",
    "    axes[idx, 1].boxplot(data, vert=False, patch_artist=True,\n",
    "                        boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "                        medianprops=dict(color='red', linewidth=2),\n",
    "                        whiskerprops=dict(color='blue'),\n",
    "                        capprops=dict(color='blue'))\n",
    "    axes[idx, 1].set_title(f'{col} - Box Plot (Outlier Detection)', fontweight='bold')\n",
    "    axes[idx, 1].set_xlabel(col)\n",
    "    axes[idx, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7acaf",
   "metadata": {},
   "source": [
    "### 2.3 Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a31a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "categorical_features = ['city', 'gender', 'relevent_experience', 'enrolled_university', \n",
    "                        'education_level', 'major_discipline', 'experience', \n",
    "                        'company_size', 'company_type', 'last_new_job']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CATEGORICAL FEATURES - UNIQUE VALUES OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cat_summary = []\n",
    "for col in categorical_features:\n",
    "    n_unique = df[col].nunique()\n",
    "    n_missing = df[col].isnull().sum()\n",
    "    missing_pct = (n_missing / len(df)) * 100\n",
    "    \n",
    "    cat_summary.append({\n",
    "        'Feature': col,\n",
    "        'Unique_Values': n_unique,\n",
    "        'Missing_Count': n_missing,\n",
    "        'Missing_%': f\"{missing_pct:.1f}%\"\n",
    "    })\n",
    "\n",
    "cat_df = pd.DataFrame(cat_summary)\n",
    "print(\"\\n\" + cat_df.to_string(index=False))\n",
    "\n",
    "# Detailed display for features with reasonable number of categories (exclude 'city')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED VALUES FOR EACH FEATURE (excluding high-cardinality features)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for col in categorical_features:\n",
    "    n_unique = df[col].nunique()\n",
    "    \n",
    "    # Skip features with too many unique values (like 'city' with 123 values)\n",
    "    if n_unique > 25:\n",
    "        print(f\"\\n Feature: '{col}'\")\n",
    "        print(f\"   Unique Values: {n_unique} (too many to display)\")\n",
    "        print(f\"   Note: High cardinality feature - will use Target Encoding\")\n",
    "        continue\n",
    "    \n",
    "    # Display all unique values for features with reasonable cardinality\n",
    "    print(f\"Feature: '{col}'\")\n",
    "    print(f\"   Unique Values: {n_unique}\")\n",
    "    \n",
    "    # Get value counts including missing\n",
    "    value_counts = df[col].value_counts(dropna=False)\n",
    "    \n",
    "    print(f\"   All Values: \", end=\"\")\n",
    "    all_values = df[col].dropna().unique().tolist()\n",
    "    \n",
    "    if n_unique <= 10:\n",
    "        # For features with ‚â§10 values, display vertically with counts\n",
    "        print()\n",
    "        for val in sorted([str(v) for v in all_values]):\n",
    "            count = value_counts.get(val, 0)\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"      ‚Ä¢ {val:30s}  ({count:>6,} samples, {pct:>5.2f}%)\")\n",
    "        \n",
    "        # Show missing values if any\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"      ‚Ä¢ [MISSING]                       ({missing_count:>6,} samples, {missing_count/len(df)*100:>5.2f}%)\")\n",
    "    else:\n",
    "        # For features with 11-25 values, display horizontally \n",
    "        print(f\"{sorted([str(v) for v in all_values])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f782ca",
   "metadata": {},
   "source": [
    "### 2.4 High-Priority Categorical Features - Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze high-priority categorical features in detail\n",
    "high_priority_features = ['gender', 'company_size', 'company_type', 'major_discipline', 'experience']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HIGH-PRIORITY CATEGORICAL FEATURES - DETAILED FREQUENCY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for col in high_priority_features:\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Feature: '{col}'\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    # Value counts\n",
    "    value_counts = df[col].value_counts(dropna=False)\n",
    "    value_pcts = df[col].value_counts(normalize=True, dropna=False) * 100\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Value': value_counts.index,\n",
    "        'Count': value_counts.values,\n",
    "        'Percentage': [f\"{pct:.2f}%\" for pct in value_pcts.values]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n   Total Unique Values: {df[col].nunique()}\")\n",
    "    print(f\"   Missing Values: {df[col].isnull().sum()} ({df[col].isnull().sum()/len(df)*100:.2f}%)\")\n",
    "    print(f\"\\n   Frequency Distribution:\")\n",
    "    print(\"   \" + \"-\" * 70)\n",
    "    \n",
    "    # Display top 15 values (or all if less than 15)\n",
    "    display_limit = min(15, len(summary_df))\n",
    "    for idx, row in summary_df.head(display_limit).iterrows():\n",
    "        val_str = str(row['Value']) if pd.notna(row['Value']) else '**MISSING**'\n",
    "        print(f\"   {val_str:30s} {row['Count']:>8,}   ({row['Percentage']:>7s})\")\n",
    "    \n",
    "    if len(summary_df) > display_limit:\n",
    "        print(f\"   ... ({len(summary_df) - display_limit} more values)\")\n",
    "    \n",
    "    # Identify rare categories (< 1% of data)\n",
    "    rare_cats = value_pcts[value_pcts < 1.0]\n",
    "    if len(rare_cats) > 0:\n",
    "        print(f\"\\n   ‚ö†Ô∏è  Rare categories (< 1%): {len(rare_cats)} values\")\n",
    "        print(f\"      Consider combining or special handling\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16af4820",
   "metadata": {},
   "source": [
    "### 2.5 Feature vs Target - Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72359f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key features' relationship with target variable\n",
    "key_features = ['relevent_experience', 'company_size', 'company_type', 'education_level']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE vs TARGET RELATIONSHIP ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for col in key_features:\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Feature: '{col}' vs Target (Job Change Intention)\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    # Create crosstab without margins first\n",
    "    crosstab = pd.crosstab(df[col], df['target'], dropna=False)\n",
    "    \n",
    "    # Calculate percentages (target=1 rate for each category)\n",
    "    target_rate = pd.crosstab(df[col], df['target'], normalize='index', dropna=False)\n",
    "    \n",
    "    # Combine into summary\n",
    "    summary_data = []\n",
    "    for cat in crosstab.index:\n",
    "        total = crosstab.loc[cat].sum()\n",
    "        target_0 = crosstab.loc[cat, 0.0] if 0.0 in crosstab.columns else 0\n",
    "        target_1 = crosstab.loc[cat, 1.0] if 1.0 in crosstab.columns else 0\n",
    "        rate_1 = target_rate.loc[cat, 1.0] * 100 if 1.0 in target_rate.columns else 0\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Category': cat,\n",
    "            'Total_Count': int(total),\n",
    "            'Target=0': int(target_0),\n",
    "            'Target=1': int(target_1),\n",
    "            'Target=1_Rate': f\"{rate_1:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    summary = pd.DataFrame(summary_data)\n",
    "    print(\"\\n\" + summary.to_string(index=False))\n",
    "    \n",
    "    # Highlight significant differences\n",
    "    rates = [float(row['Target=1_Rate'].rstrip('%')) for row in summary_data]\n",
    "    avg_rate = np.mean(rates)\n",
    "    print(f\"\\n   Average Target=1 Rate: {avg_rate:.2f}%\")\n",
    "    \n",
    "    high_rate = [row['Category'] for row, rate in zip(summary_data, rates) if rate > avg_rate + 5]\n",
    "    low_rate = [row['Category'] for row, rate in zip(summary_data, rates) if rate < avg_rate - 5]\n",
    "    \n",
    "    if high_rate:\n",
    "        print(f\"Higher likelihood of job change: {high_rate}\")\n",
    "    if low_rate:\n",
    "        print(f\"Lower likelihood of job change: {low_rate}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313ab56",
   "metadata": {},
   "source": [
    "### 2.6 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f7f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Duplicate rows check\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Duplicate Rows: {duplicate_count}\")\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Found {duplicate_count} duplicate rows\")\n",
    "    print(f\"Action: Review and consider removing duplicates\")\n",
    "else:\n",
    "    print(\"No duplicate rows found\")\n",
    "\n",
    "# 2. Duplicate enrollee_id check\n",
    "duplicate_ids = df['enrollee_id'].duplicated().sum()\n",
    "print(f\"Duplicate Enrollee IDs: {duplicate_ids}\")\n",
    "if duplicate_ids > 0:\n",
    "    print(f\"Found {duplicate_ids} duplicate IDs - possible data entry errors\")\n",
    "else:\n",
    "    print(\"All enrollee IDs are unique\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31b303",
   "metadata": {},
   "source": [
    "### 2.8 Summary of Data Analysis Findings\n",
    "\n",
    "**Key Insights from Exploratory Data Analysis:**\n",
    "\n",
    "**Target Variable**\n",
    "- Check class balance and imbalance ratio\n",
    "- Determine if sampling/weighting strategies are needed\n",
    "\n",
    "**Numerical Features (2 features)**\n",
    "- **Distribution shape**: Skewness & Kurtosis analysis\n",
    "- **Outlier detection**: IQR method for anomaly identification\n",
    "- **Statistical summary**: Mean, median, std, range\n",
    "\n",
    "**Categorical Features (10 features)**\n",
    "- **Unique values count**: Understanding cardinality\n",
    "- **Missing data**: Identify features requiring imputation\n",
    "- **Frequency distribution**: Find rare/dominant categories\n",
    "- **Feature-Target relationship**: Identify predictive patterns\n",
    "\n",
    "**Data Quality**\n",
    "- Duplicate records check\n",
    "- ID uniqueness validation\n",
    "- Format consistency (e.g., `company_size` \"10/49\" issue)\n",
    "- Range validation for numerical features\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:** Based on findings, proceed to Data Visualisation and then Data Cleaning (Section 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedcba3",
   "metadata": {},
   "source": [
    "### 2.9 Key Data Visualisations\n",
    "\n",
    "In this section, we visualize the relationships between key features and the target variable to gain insights into what factors influence job change decisions.\n",
    "\n",
    "**Visualisation Strategy:**\n",
    "1. **Categorical Features vs Target** - Compare job change rates across categories\n",
    "2. **Numerical Features Distribution** - Compare distributions between two target classes\n",
    "3. **Key Insights** - Summarize findings from visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA VISUALISATION: Feature Relationships with Target Variable\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1af23",
   "metadata": {},
   "source": [
    "#### Visualisation 1: Key Categorical Features vs Job Change Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33060ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key categorical features for visualization\n",
    "key_cat_features = ['relevent_experience', 'company_size', 'company_type', 'education_level']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Job Change Rate by Key Categorical Features', fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "for idx, feature in enumerate(key_cat_features):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Calculate target rate for each category\n",
    "    crosstab = pd.crosstab(df[feature], df['target'])\n",
    "    target_rate = (crosstab[1.0] / crosstab.sum(axis=1) * 100).sort_values(ascending=False)\n",
    "    \n",
    "    # Create bar plot\n",
    "    colors = plt.cm.RdYlGn_r(target_rate / 100)\n",
    "    bars = ax.barh(range(len(target_rate)), target_rate.values, color=colors, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_yticks(range(len(target_rate)))\n",
    "    ax.set_yticklabels(target_rate.index, fontsize=10)\n",
    "    ax.set_xlabel('Job Change Rate (%)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{feature}', fontsize=12, fontweight='bold')\n",
    "    ax.axvline(df['target'].mean() * 100, color='red', linestyle='--', linewidth=2, label='Overall Average')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (v, count) in enumerate(zip(target_rate.values, crosstab.sum(axis=1)[target_rate.index])):\n",
    "        ax.text(v + 1, i, f'{v:.1f}% (n={count:,})', va='center', fontsize=9)\n",
    "    \n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_xlim(0, max(target_rate.values) * 1.15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"1. Relevant Experience: Candidates WITH relevant experience have LOWER job change rates\")\n",
    "print(\"2. Company Size: Smaller companies (<10) show higher job change tendency\")\n",
    "print(\"3. Company Type: Pvt Ltd companies have highest retention (lowest change rate)\")\n",
    "print(\"4. Education Level: Graduate level shows balanced job change behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c4dec1",
   "metadata": {},
   "source": [
    "#### Visualisation 2: Experience Level Distribution by Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd607dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize experience distribution for both target classes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Experience Distribution: Job Changers vs Non-Changers', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Convert experience to numeric for plotting\n",
    "exp_mapping = {\n",
    "    '<1': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5,\n",
    "    '6': 6, '7': 7, '8': 8, '9': 9, '10': 10,\n",
    "    '11': 11, '12': 12, '13': 13, '14': 14, '15': 15,\n",
    "    '16': 16, '17': 17, '18': 18, '19': 19, '20': 20, '>20': 21\n",
    "}\n",
    "\n",
    "df_temp = df.copy()\n",
    "df_temp['experience_numeric'] = df_temp['experience'].map(exp_mapping)\n",
    "\n",
    "# Plot 1: Histogram comparison\n",
    "for target_val, color, label in [(0, '#3498db', 'Not Looking for Change'), \n",
    "                                   (1, '#e74c3c', 'Looking for Change')]:\n",
    "    data = df_temp[df_temp['target'] == target_val]['experience_numeric'].dropna()\n",
    "    axes[0].hist(data, bins=22, alpha=0.6, label=label, color=color, edgecolor='black')\n",
    "\n",
    "axes[0].set_xlabel('Years of Experience', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Experience Distribution Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Box plot comparison\n",
    "df_temp.boxplot(column='experience_numeric', by='target', ax=axes[1], patch_artist=True)\n",
    "axes[1].set_xlabel('Target (0=Not Looking, 1=Looking)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Years of Experience', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Experience Distribution by Target', fontsize=13, fontweight='bold')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Average experience (Not looking for change): {df_temp[df_temp['target']==0]['experience_numeric'].mean():.2f} years\")\n",
    "print(f\"Average experience (Looking for change):     {df_temp[df_temp['target']==1]['experience_numeric'].mean():.2f} years\")\n",
    "print(\"Job changers tend to have slightly LOWER average experience\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a928c",
   "metadata": {},
   "source": [
    "#### Visualisation 3: Training Hours and City Development Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize numerical features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Numerical Features: Training Hours & City Development Index', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Training Hours by Target\n",
    "for target_val, color, label in [(0, '#3498db', 'Not Looking (0)'), \n",
    "                                   (1, '#e74c3c', 'Looking (1)')]:\n",
    "    data = df[df['target'] == target_val]['training_hours']\n",
    "    axes[0].hist(data, bins=30, alpha=0.6, label=label, color=color, edgecolor='black')\n",
    "\n",
    "axes[0].set_xlabel('Training Hours', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Training Hours Distribution by Target', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].axvline(df[df['target']==0]['training_hours'].mean(), color='#3498db', linestyle='--', linewidth=2, label='Mean (0)')\n",
    "axes[0].axvline(df[df['target']==1]['training_hours'].mean(), color='#e74c3c', linestyle='--', linewidth=2, label='Mean (1)')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: City Development Index by Target\n",
    "df.boxplot(column='city_development_index', by='target', ax=axes[1], patch_artist=True)\n",
    "axes[1].set_xlabel('Target (0=Not Looking, 1=Looking)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('City Development Index', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('City Development Index by Target', fontsize=13, fontweight='bold')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Average training hours (Not looking): {df[df['target']==0]['training_hours'].mean():.1f} hours\")\n",
    "print(f\"Average training hours (Looking):     {df[df['target']==1]['training_hours'].mean():.1f} hours\")\n",
    "print(f\"Average CDI (Not looking): {df[df['target']==0]['city_development_index'].mean():.3f}\")\n",
    "print(f\"Average CDI (Looking):     {df[df['target']==1]['city_development_index'].mean():.3f}\")\n",
    "print(\"\\nCandidates in MORE developed cities are LESS likely to change jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02843f68",
   "metadata": {},
   "source": [
    "#### Summary of Visualisation Insights\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "1. **Relevant Experience Impact**\n",
    "   - Candidates WITHOUT relevant experience are MORE likely to seek job changes\n",
    "   - This suggests career dissatisfaction or seeking better fit\n",
    "\n",
    "2. **Company Size Effect**\n",
    "   - Smaller companies (<10 employees) have higher turnover rates\n",
    "   - Larger companies (1000+) show better retention\n",
    "\n",
    "3. **Experience Level Paradox**\n",
    "   - Job changers have slightly LOWER average experience\n",
    "   - Mid-career professionals (5-10 years) show highest mobility\n",
    "\n",
    "4. **City Development Factor**\n",
    "   - Candidates in less developed cities are MORE likely to change jobs\n",
    "   - Economic opportunities may drive this pattern\n",
    "\n",
    "5. **Training Hours**\n",
    "   - Similar training hours across both groups\n",
    "   - Training alone doesn't predict job change behavior\n",
    "\n",
    "**Implications for Modeling:**\n",
    "- `relevent_experience`, `company_size`, and `city_development_index` are likely strong predictors\n",
    "- `experience` shows non-linear relationship (requires careful feature engineering)\n",
    "- `training_hours` may be less predictive (but still worth including)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8fc5a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  3. Data Cleaning & Feature Engineering Strategy\n",
    "\n",
    "Based on the comprehensive data analysis above, here is the detailed processing strategy for each of the 14 features:\n",
    "\n",
    "###  3.1 Strategy Overview Table\n",
    "\n",
    "| # | Feature | Type | Action | Priority | Reason |\n",
    "|---|---------|------|--------|----------|--------|\n",
    "| 1 | `enrollee_id` | ID | **DELETE** | üî¥ High | No predictive value - just an identifier |\n",
    "| 2 | `city` | Categorical (123 values) | **Target Encoding** | üü° Medium | Too many categories for One-Hot |\n",
    "| 3 | `city_development_index` | Numerical | **Keep as-is** | üü¢ Low | Already numeric, no missing values |\n",
    "| 4 | `gender` | Categorical (3 values) | **Fill Missing + One-Hot** | üü° Medium | 23% missing, create \"Unknown\" category |\n",
    "| 5 | `relevent_experience` | Binary | **Label Encoding** | üü¢ Low | No missing, convert to 0/1 |\n",
    "| 6 | `enrolled_university` | Categorical (3 values) | **Fill Missing + One-Hot** | üü¢ Low | 2% missing, 3 clear categories |\n",
    "| 7 | `education_level` | Ordinal (5 values) | **Ordinal Encoding** | üü° Medium | 2% missing, has natural order |\n",
    "| 8 | `major_discipline` | Categorical (6 values) | **Fill Missing + One-Hot** | üü° Medium | 15% missing, may correlate with education |\n",
    "| 9 | `experience` | Ordinal (22 values) | **Ordinal Encoding** | üî¥ High | Convert to numeric years (e.g., \"<1\"‚Üí0, \">20\"‚Üí21) |\n",
    "| 10 | `company_size` | Ordinal (8 values) | **Fix Format + Ordinal** | üî¥ High | 31% missing, FIX \"10/49\" ‚Üí \"10-49\" |\n",
    "| 11 | `company_type` | Categorical (6 values) | **Fill Missing + One-Hot** | üü° Medium | 32% missing, create \"Unknown\" |\n",
    "| 12 | `last_new_job` | Ordinal (6 values) | **Ordinal Encoding** | üü¢ Low | 2% missing, convert to numeric |\n",
    "| 13 | `training_hours` | Numerical | **Keep as-is** | üü¢ Low | Already numeric, no missing values |\n",
    "| 14 | `target` | Binary | **Keep as-is** | N/A | Target variable - no transformation needed |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e6ee55",
   "metadata": {},
   "source": [
    "### 3.2 Data Clean and Base Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68762c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataframe for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(f\"Original dataset shape: {df_clean.shape}\")\n",
    "print(f\"Starting data cleaning process...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de077cc9",
   "metadata": {},
   "source": [
    "#### 1. `enrollee_id`\n",
    "Delete the column `enrollee_id` because it is not interesting for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b8ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 1: enrollee_id\n",
    "print(\"=\"*70)\n",
    "print(\"Processing Feature 1: enrollee_id\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_count = df_clean['enrollee_id'].isna().sum()\n",
    "print(f\"\\nMissing before: {missing_count} ({missing_count/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Delete the column (no predictive value)\n",
    "df_clean = df_clean.drop('enrollee_id', axis=1)\n",
    "\n",
    "print(f\"\\nFeature 1 processed: enrollee_id\")\n",
    "print(f\"   Action: Deleted (identifier column)\")\n",
    "print(f\"   New shape: {df_clean.shape}\")\n",
    "print(f\"   Columns remaining: {df_clean.shape[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364fe2eb",
   "metadata": {},
   "source": [
    "#### 2. `company_size`\n",
    "**Action:** Fix format error ('10/49'‚Üí'10-49'), create missing indicator, fill with median, ordinal encoding (0-7)  \n",
    "**Reason:** 31% missing + format issue; ordinal relationship exists (company scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 2: company_size\n",
    "print(\"=\"*70)\n",
    "print(\"Processing Feature 2: company_size\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_count = df_clean['company_size'].isna().sum()\n",
    "print(f\"\\nMissing before: {missing_count} ({missing_count/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Step 1: Fix formatting error\n",
    "print(\"\\nStep 1: Fix formatting error '10/49' -> '10-49'\")\n",
    "df_clean['company_size'] = df_clean['company_size'].replace('10/49', '10-49')\n",
    "print(f\"   Fixed {df_clean['company_size'].value_counts().get('10-49', 0):,} instances\")\n",
    "\n",
    "# Step 2: Create missing indicator\n",
    "print(\"\\nStep 2: Create missing indicator feature\")\n",
    "df_clean['company_size_missing'] = df_clean['company_size'].isna().astype(int)\n",
    "print(f\"   Created 'company_size_missing' (1=missing, 0=known)\")\n",
    "\n",
    "# Step 3: Fill missing with median category\n",
    "print(\"\\nStep 3: Fill missing with median category\")\n",
    "median_category = '50-99'\n",
    "df_clean['company_size'] = df_clean['company_size'].fillna(median_category)\n",
    "print(f\"   Filled {missing_count:,} missing values with '{median_category}'\")\n",
    "print(f\"   Missing after: {df_clean['company_size'].isna().sum()}\")\n",
    "\n",
    "# Step 4: Ordinal encoding\n",
    "print(\"\\nStep 4: Ordinal encoding (small to large)\")\n",
    "size_order = {\n",
    "    '<10': 0, '10-49': 1, '50-99': 2, '100-500': 3,\n",
    "    '500-999': 4, '1000-4999': 5, '5000-9999': 6, '10000+': 7\n",
    "}\n",
    "df_clean['company_size'] = df_clean['company_size'].map(size_order)\n",
    "\n",
    "print(\"   Encoding mapping:\")\n",
    "for key, value in size_order.items():\n",
    "    count = (df_clean['company_size'] == value).sum()\n",
    "    print(f\"      {key:12s} -> {value}  ({count:,} samples)\")\n",
    "\n",
    "print(f\"\\nFeature 2 processed: company_size + company_size_missing\")\n",
    "print(f\"   company_size: Type={df_clean['company_size'].dtype}, Range=[{df_clean['company_size'].min()}, {df_clean['company_size'].max()}]\")\n",
    "print(f\"   company_size_missing: {df_clean['company_size_missing'].value_counts().to_dict()}\")\n",
    "print(f\"   Shape: {df_clean.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a8048",
   "metadata": {},
   "source": [
    "#### 3. `company_type`\n",
    "**Action:** Fill missing with 'Unknown', One-Hot encoding  \n",
    "**Reason:** 32% missing; 6 nominal categories (no natural order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f0ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 3: company_type\n",
    "print(\"=\"*70)\n",
    "print(\"Processing Feature 3: company_type\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_count = df_clean['company_type'].isna().sum()\n",
    "print(f\"\\nMissing before: {missing_count} ({missing_count/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Step 1: Fill missing values\n",
    "print(\"\\nStep 1: Fill missing values with 'Unknown'\")\n",
    "df_clean['company_type'] = df_clean['company_type'].fillna('Unknown')\n",
    "print(f\"   Filled {missing_count:,} missing values with 'Unknown'\")\n",
    "print(f\"   Missing after: {df_clean['company_type'].isna().sum()}\")\n",
    "\n",
    "# Step 2: One-Hot encoding\n",
    "print(\"\\nStep 2: One-Hot encoding\")\n",
    "print(f\"   Categories: {sorted(df_clean['company_type'].unique())}\")\n",
    "\n",
    "company_type_dummies = pd.get_dummies(df_clean['company_type'], prefix='company_type', drop_first=False)\n",
    "df_clean = pd.concat([df_clean, company_type_dummies], axis=1)\n",
    "df_clean = df_clean.drop('company_type', axis=1)\n",
    "\n",
    "print(f\"   Created {len(company_type_dummies.columns)} binary columns:\")\n",
    "for col in sorted(company_type_dummies.columns):\n",
    "    count = df_clean[col].sum()\n",
    "    print(f\"      {col:35s}: {count:,} samples\")\n",
    "\n",
    "print(f\"\\nFeature 3 processed: company_type\")\n",
    "print(f\"   New columns added: {len(company_type_dummies.columns)}\")\n",
    "print(f\"   Shape: {df_clean.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f1eb25",
   "metadata": {},
   "source": [
    "#### 4. `gender`\n",
    "**Action:** Fill missing with 'Unknown', One-Hot encoding  \n",
    "**Reason:** 23% missing; 3 nominal categories (Male/Female/Other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b54b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 4: gender\n",
    "print(\"=\"*70)\n",
    "print(\"Processing Feature 4: gender\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_count = df_clean['gender'].isna().sum()\n",
    "print(f\"\\nMissing before: {missing_count} ({missing_count/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Step 1: Fill missing values\n",
    "print(\"\\nStep 1: Fill missing values with 'Unknown'\")\n",
    "df_clean['gender'] = df_clean['gender'].fillna('Unknown')\n",
    "print(f\"   Filled {missing_count:,} missing values with 'Unknown'\")\n",
    "print(f\"   Missing after: {df_clean['gender'].isna().sum()}\")\n",
    "\n",
    "# Step 2: One-Hot encoding\n",
    "print(\"\\nStep 2: One-Hot encoding\")\n",
    "gender_dummies = pd.get_dummies(df_clean['gender'], prefix='gender', drop_first=False)\n",
    "df_clean = pd.concat([df_clean, gender_dummies], axis=1)\n",
    "df_clean = df_clean.drop('gender', axis=1)\n",
    "\n",
    "print(f\"   Created {len(gender_dummies.columns)} binary columns:\")\n",
    "for col in sorted(gender_dummies.columns):\n",
    "    count = df_clean[col].sum()\n",
    "    print(f\"      {col:35s}: {count:,} samples\")\n",
    "\n",
    "print(f\"\\nFeature 4 processed: gender\")\n",
    "print(f\"   Shape: {df_clean.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0acf3b",
   "metadata": {},
   "source": [
    "#### 5. `major_discipline`\n",
    "**Action:** Fill missing with 'Unknown', One-Hot encoding  \n",
    "**Reason:** 15% missing; 6 nominal categories (STEM, Business, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9528fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 5: major_discipline\n",
    "print(\"=\"*70)\n",
    "print(\"Processing Feature 5: major_discipline\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_count = df_clean['major_discipline'].isna().sum()\n",
    "print(f\"\\nMissing before: {missing_count} ({missing_count/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Step 1: Fill missing values\n",
    "print(\"\\nStep 1: Fill missing values with 'Unknown'\")\n",
    "df_clean['major_discipline'] = df_clean['major_discipline'].fillna('Unknown')\n",
    "print(f\"   Filled {missing_count:,} missing values with 'Unknown'\")\n",
    "print(f\"   Missing after: {df_clean['major_discipline'].isna().sum()}\")\n",
    "\n",
    "# Step 2: One-Hot encoding\n",
    "print(\"\\nStep 2: One-Hot encoding\")\n",
    "major_dummies = pd.get_dummies(df_clean['major_discipline'], prefix='major', drop_first=False)\n",
    "df_clean = pd.concat([df_clean, major_dummies], axis=1)\n",
    "df_clean = df_clean.drop('major_discipline', axis=1)\n",
    "\n",
    "print(f\"   Created {len(major_dummies.columns)} binary columns:\")\n",
    "for col in sorted(major_dummies.columns):\n",
    "    count = df_clean[col].sum()\n",
    "    print(f\"      {col:35s}: {count:,} samples\")\n",
    "\n",
    "print(f\"\\nFeature 5 processed: major_discipline\")\n",
    "print(f\"   Shape: {df_clean.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14bb2f5",
   "metadata": {},
   "source": [
    "#### 6. `experience`\n",
    "**Action:** Fill missing with median, ordinal encoding ('<1'‚Üí0, '20'‚Üí20, '>20'‚Üí21)  \n",
    "**Reason:** Few missing values; clear ordinal relationship (years of experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 6: experience\n",
    "print(\"=\"*70)\n",
    "print(\"Processing Feature 6: experience\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_count = df_clean['experience'].isna().sum()\n",
    "print(f\"\\nMissing before: {missing_count} ({missing_count/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Define ordinal mapping\n",
    "exp_mapping = {\n",
    "    '<1': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5,\n",
    "    '6': 6, '7': 7, '8': 8, '9': 9, '10': 10,\n",
    "    '11': 11, '12': 12, '13': 13, '14': 14, '15': 15,\n",
    "    '16': 16, '17': 17, '18': 18, '19': 19, '20': 20, '>20': 21\n",
    "}\n",
    "\n",
    "# Step 1: Fill missing with median\n",
    "print(\"\\nStep 1: Fill missing with median\")\n",
    "temp_numeric = df_clean['experience'].dropna().map(exp_mapping)\n",
    "median_value = temp_numeric.median()\n",
    "median_key = min(exp_mapping.items(), key=lambda x: abs(x[1] - median_value))[0]\n",
    "df_clean['experience'] = df_clean['experience'].fillna(median_key)\n",
    "print(f\"   Filled {missing_count:,} missing values with median '{median_key}' (value={exp_mapping[median_key]})\")\n",
    "\n",
    "# Step 2: Ordinal encoding\n",
    "print(\"\\nStep 2: Ordinal encoding\")\n",
    "df_clean['experience'] = df_clean['experience'].map(exp_mapping)\n",
    "print(f\"   Encoded: '<1'->0, '1'->1, ..., '>20'->21\")\n",
    "\n",
    "print(f\"\\nFeature 6 processed: experience\")\n",
    "print(f\"   Type: {df_clean['experience'].dtype}, Range: [{df_clean['experience'].min()}, {df_clean['experience'].max()}] years\")\n",
    "print(f\"   Missing after: {df_clean['experience'].isna().sum()}\")\n",
    "print(f\"   Shape: {df_clean.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb66db8",
   "metadata": {},
   "source": [
    "#### 7. `education_level`\n",
    "**Action:** Fill missing with mode, ordinal encoding (Primary‚Üí1 to PhD‚Üí5)  \n",
    "**Reason:** 2% missing; clear educational hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2418968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 7: education_level\n",
    "print(\"=\"*70)\n",
    "print(\"Processing Feature 7: education_level\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_count = df_clean['education_level'].isna().sum()\n",
    "print(f\"\\nMissing before: {missing_count} ({missing_count/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Step 1: Fill missing with mode\n",
    "print(\"\\nStep 1: Fill missing with mode\")\n",
    "mode_value = df_clean['education_level'].mode()[0]\n",
    "df_clean['education_level'] = df_clean['education_level'].fillna(mode_value)\n",
    "print(f\"   Filled {missing_count:,} missing values with mode '{mode_value}'\")\n",
    "\n",
    "# Step 2: Ordinal encoding\n",
    "print(\"\\nStep 2: Ordinal encoding (educational hierarchy)\")\n",
    "edu_mapping = {\n",
    "    'Primary School': 1,\n",
    "    'High School': 2,\n",
    "    'Graduate': 3,\n",
    "    'Masters': 4,\n",
    "    'Phd': 5\n",
    "}\n",
    "df_clean['education_level'] = df_clean['education_level'].map(edu_mapping)\n",
    "print(f\"   Encoded: Primary School->1, High School->2, Graduate->3, Masters->4, PhD->5\")\n",
    "\n",
    "print(f\"\\nFeature 7 processed: education_level\")\n",
    "print(f\"   Type: {df_clean['education_level'].dtype}, Range: [{df_clean['education_level'].min()}, {df_clean['education_level'].max()}]\")\n",
    "print(f\"   Missing after: {df_clean['education_level'].isna().sum()}\")\n",
    "print(f\"   Shape: {df_clean.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c317da",
   "metadata": {},
   "source": [
    "#### 8. `enrolled_university`\n",
    "**Action:** Fill missing with 'no_enrollment', One-Hot encoding  \n",
    "**Reason:** 2% missing; 3 nominal categories (enrollment status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710498d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 8: enrolled_university\n",
    "print(\"=\"*70)\n",
    "print(\"Processing Feature 8: enrolled_university\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_count = df_clean['enrolled_university'].isna().sum()\n",
    "print(f\"\\nMissing before: {missing_count} ({missing_count/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Step 1: Fill missing values\n",
    "print(\"\\nStep 1: Fill missing values with 'no_enrollment'\")\n",
    "df_clean['enrolled_university'] = df_clean['enrolled_university'].fillna('no_enrollment')\n",
    "print(f\"   Missing after: {df_clean['enrolled_university'].isna().sum()}\")\n",
    "\n",
    "# Step 2: One-Hot encoding\n",
    "print(\"\\nStep 2: One-Hot encoding\")\n",
    "enrolled_dummies = pd.get_dummies(df_clean['enrolled_university'], prefix='enrolled', drop_first=False)\n",
    "df_clean = pd.concat([df_clean, enrolled_dummies], axis=1)\n",
    "df_clean = df_clean.drop('enrolled_university', axis=1)\n",
    "\n",
    "print(f\"   Created {len(enrolled_dummies.columns)} binary columns:\")\n",
    "for col in sorted(enrolled_dummies.columns):\n",
    "    count = df_clean[col].sum()\n",
    "    print(f\"      {col:35s}: {count:,} samples\")\n",
    "\n",
    "print(f\"\\nFeature 8 processed: enrolled_university\")\n",
    "print(f\"   Shape: {df_clean.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d400a9b",
   "metadata": {},
   "source": [
    "#### 9. `relevent_experience`\n",
    "**Action:** Binary encoding (Has‚Üí1, No‚Üí0)  \n",
    "**Reason:** No missing values; binary feature with clear yes/no distinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6124f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 9: relevent_experience\n",
    "print(\"=\"*70)\n",
    "print(\"Processing Feature 9: relevent_experience\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_count = df_clean['relevent_experience'].isna().sum()\n",
    "print(f\"\\nMissing before: {missing_count} ({missing_count/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Binary encoding\n",
    "print(\"\\nStep 1: Binary encoding\")\n",
    "df_clean['relevent_experience'] = df_clean['relevent_experience'].map({\n",
    "    'Has relevent experience': 1,\n",
    "    'No relevent experience': 0\n",
    "})\n",
    "print(f\"   Encoded: 'Has relevent experience'->1, 'No relevent experience'->0\")\n",
    "\n",
    "print(f\"\\nFeature 9 processed: relevent_experience\")\n",
    "print(f\"   Type: {df_clean['relevent_experience'].dtype}\")\n",
    "print(f\"   Distribution: {df_clean['relevent_experience'].value_counts().to_dict()}\")\n",
    "print(f\"   Missing after: {df_clean['relevent_experience'].isna().sum()}\")\n",
    "print(f\"   Shape: {df_clean.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe80a7",
   "metadata": {},
   "source": [
    "#### 10. `last_new_job`\n",
    "**Action:** Fill missing with mode, ordinal encoding ('never'‚Üí0 to '>4'‚Üí5)  \n",
    "**Reason:** 2% missing; ordinal relationship (recency of job change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31196115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 10: last_new_job\n",
    "print(\"=\"*70)\n",
    "print(\"Processing Feature 10: last_new_job\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_count = df_clean['last_new_job'].isna().sum()\n",
    "print(f\"\\nMissing before: {missing_count} ({missing_count/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Step 1: Fill missing with mode\n",
    "print(\"\\nStep 1: Fill missing with mode\")\n",
    "mode_value = df_clean['last_new_job'].mode()[0]\n",
    "df_clean['last_new_job'] = df_clean['last_new_job'].fillna(mode_value)\n",
    "print(f\"   Filled {missing_count:,} missing values with mode '{mode_value}'\")\n",
    "\n",
    "# Step 2: Ordinal encoding\n",
    "print(\"\\nStep 2: Ordinal encoding (job change recency)\")\n",
    "job_mapping = {'never': 0, '1': 1, '2': 2, '3': 3, '4': 4, '>4': 5}\n",
    "df_clean['last_new_job'] = df_clean['last_new_job'].map(job_mapping)\n",
    "print(f\"   Encoded: 'never'->0, '1'->1, '2'->2, '3'->3, '4'->4, '>4'->5\")\n",
    "\n",
    "print(f\"\\nFeature 10 processed: last_new_job\")\n",
    "print(f\"   Type: {df_clean['last_new_job'].dtype}, Range: [{df_clean['last_new_job'].min()}, {df_clean['last_new_job'].max()}]\")\n",
    "print(f\"   Missing after: {df_clean['last_new_job'].isna().sum()}\")\n",
    "print(f\"   Shape: {df_clean.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d861987d",
   "metadata": {},
   "source": [
    "#### 11. `city`\n",
    "**Action:** Target encoding after dataset split (to prevent data leakage)  \n",
    "**Reason:** High cardinality (123 unique values); avoid curse of dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cb592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 11: city\n",
    "print(\"=\"*70)\n",
    "print(\"Processing Feature 11: city\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_count = df_clean['city'].isna().sum()\n",
    "print(f\"\\nMissing before: {missing_count} ({missing_count/len(df_clean)*100:.2f}%)\")\n",
    "print(f\"Unique cities: {df_clean['city'].nunique()}\")\n",
    "\n",
    "print(\"\\nNote: Target encoding will be applied AFTER dataset split\")\n",
    "print(\"   Reason: Prevent data leakage (target info must not leak from train to test)\")\n",
    "print(\"   High cardinality (123 cities) - unsuitable for One-Hot encoding\")\n",
    "\n",
    "print(f\"\\nFeature 11 prepared: city (encoding deferred)\")\n",
    "print(f\"   Will encode after train/validation/test split\")\n",
    "print(f\"   Shape: {df_clean.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a818fd",
   "metadata": {},
   "source": [
    "#### 12-13. `city_development_index` & `training_hours`\n",
    "**Action:** Keep as-is (no transformation needed)  \n",
    "**Reason:** Already numeric, no missing values, ready for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features 12-13: city_development_index & training_hours\n",
    "print(\"=\"*70)\n",
    "print(\"Processing Features 12-13: Numeric Features\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Feature 12: city_development_index\n",
    "missing_count_cdi = df_clean['city_development_index'].isna().sum()\n",
    "print(f\"\\nFeature 12: city_development_index\")\n",
    "print(f\"   Missing: {missing_count_cdi} ({missing_count_cdi/len(df_clean)*100:.2f}%)\")\n",
    "print(f\"   Type: {df_clean['city_development_index'].dtype}\")\n",
    "print(f\"   Range: [{df_clean['city_development_index'].min():.3f}, {df_clean['city_development_index'].max():.3f}]\")\n",
    "print(f\"   Already numeric, no transformation needed\")\n",
    "\n",
    "# Feature 13: training_hours\n",
    "missing_count_th = df_clean['training_hours'].isna().sum()\n",
    "print(f\"\\nFeature 13: training_hours\")\n",
    "print(f\"   Missing: {missing_count_th} ({missing_count_th/len(df_clean)*100:.2f}%)\")\n",
    "print(f\"   Type: {df_clean['training_hours'].dtype}\")\n",
    "print(f\"   Range: [{df_clean['training_hours'].min()}, {df_clean['training_hours'].max()}] hours\")\n",
    "print(f\"   Already numeric, no transformation needed\")\n",
    "\n",
    "print(f\"\\nFeatures 12-13 processed: city_development_index & training_hours\")\n",
    "print(f\"   Shape: {df_clean.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Check cleaned data quality\n",
    "print(\"=\"*80)\n",
    "print(\"DATA CLEANING VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Shape comparison\n",
    "print(\"\\n1. Dataset Shape:\")\n",
    "print(f\"   Original: {df.shape}\")\n",
    "print(f\"   Cleaned:  {df_clean.shape}\")\n",
    "print(f\"   Rows preserved: {df_clean.shape[0]} (100%)\")\n",
    "print(f\"   Columns changed: {df.shape[1]} ‚Üí {df_clean.shape[1]} (due to One-Hot encoding)\")\n",
    "\n",
    "# 2. Missing values check\n",
    "print(\"\\n2. Missing Values:\")\n",
    "missing_after = df_clean.isnull().sum().sum()\n",
    "if missing_after == 0:\n",
    "    print(f\"   No missing values! All {df_clean.shape[0] * df_clean.shape[1]:,} cells are filled\")\n",
    "else:\n",
    "    print(f\"   WARNING: {missing_after} missing values found!\")\n",
    "    print(df_clean.isnull().sum()[df_clean.isnull().sum() > 0])\n",
    "\n",
    "# 3. Data types check\n",
    "print(\"\\n3. Data Types:\")\n",
    "dtypes_summary = df_clean.dtypes.value_counts()\n",
    "print(f\"   {dtypes_summary.to_dict()}\")\n",
    "non_numeric = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "if len(non_numeric) == 0:\n",
    "    print(f\"   All features are numeric (ready for modeling)\")\n",
    "else:\n",
    "    print(f\"   Non-numeric columns found: {non_numeric}\")\n",
    "\n",
    "# 4. Feature list\n",
    "print(\"\\n4. Final Feature List:\")\n",
    "feature_cols = [col for col in df_clean.columns if col != 'target']\n",
    "print(f\"   Total features: {len(feature_cols)}\")\n",
    "print(f\"   Features: {', '.join(sorted(feature_cols)[:10])}...\")\n",
    "\n",
    "# 5. Display sample\n",
    "print(\"\\n5. Sample of Cleaned Data (first 3 rows):\")\n",
    "print(\"=\"*80)\n",
    "display(df_clean.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA CLEANING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cfbaf2",
   "metadata": {},
   "source": [
    "## 4. Dataset Split & Target Encoding (5-Fold Cross-Validation Strategy)\n",
    "\n",
    "**Overview:**\n",
    "This section implements a 5-Fold Stratified Cross-Validation strategy to maximize data utilization and improve model evaluation reliability.\n",
    "\n",
    "**Key Steps:**\n",
    "1. **Separate test set (20%)** - Hold out for final evaluation\n",
    "2. **Setup 5-Fold CV on remaining 80%** - For model training and validation\n",
    "3. **Target encode `city` feature** - Handle high cardinality without data leakage\n",
    "4. **Verify data quality** - Ensure all features are numeric and ready\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52894146",
   "metadata": {},
   "source": [
    "### 4.1 Prepare Features and Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43153f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df_clean.drop('target', axis=1)\n",
    "y = df_clean['target'].astype(int)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Dataset Preparation\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Feature matrix X: {X.shape}\")\n",
    "print(f\"Target variable y: {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Class 0 (Not looking for change): {(y==0).sum():,} ({(y==0).sum()/len(y)*100:.2f}%)\")\n",
    "print(f\"  Class 1 (Looking for change):     {(y==1).sum():,} ({(y==1).sum()/len(y)*100:.2f}%)\")\n",
    "print(f\"  Class ratio: {(y==0).sum()/(y==1).sum():.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ed13c",
   "metadata": {},
   "source": [
    "### 4.2 Dataset Split Strategy: 5-Fold Stratified Cross-Validation\n",
    "\n",
    "**Why 5-Fold CV?**\n",
    "- **Better data utilization**: 80% used for training (vs 60% in 3-way split)\n",
    "- **More reliable evaluation**: Each sample serves as validation once, averaged over 5 folds\n",
    "- **Reduced variance**: Multiple validation results -> more stable performance estimate\n",
    "\n",
    "**Split Strategy:**\n",
    "1. **Test set (20%)**: Hold out completely, only used for final evaluation\n",
    "2. **Train+Validation (80%)**: Split into 5 folds for cross-validation\n",
    "   - Each fold: 4 parts training (64%) + 1 part validation (16%)\n",
    "   - Repeat 5 times, every sample gets validated once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Separate test set (20%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,           # 20% for test set\n",
    "    stratify=y,               # Stratified sampling to preserve class ratio\n",
    "    random_state=22207256           # Fixed seed for reproducibility\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Step 1: Test Set Separation\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train+Validation set: {X_train_val.shape[0]:,} samples ({X_train_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set:             {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify class distribution in test set\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(f\"  Class 0: {(y_test==0).sum():,} ({(y_test==0).sum()/len(y_test)*100:.2f}%)\")\n",
    "print(f\"  Class 1: {(y_test==1).sum():,} ({(y_test==1).sum()/len(y_test)*100:.2f}%)\")\n",
    "print(f\"  Ratio: {(y_test==0).sum()/(y_test==1).sum():.2f}:1\")\n",
    "\n",
    "# Step 2: Setup 5-Fold Stratified Cross-Validation on Train+Val set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 2: Setup 5-Fold Stratified Cross-Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=22307256)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Number of folds: 5\")\n",
    "print(f\"  Total samples for CV: {X_train_val.shape[0]:,}\")\n",
    "print(f\"  Samples per fold (validation): ~{X_train_val.shape[0]//5:,} ({100/5:.1f}%)\")\n",
    "print(f\"  Samples per fold (training): ~{X_train_val.shape[0]*4//5:,} ({100*4/5:.1f}%)\")\n",
    "\n",
    "# Verify fold quality\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Fold Quality Verification:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Fold':<8} {'Train Samples':<15} {'Val Samples':<15} {'Class 0 (Val)':<15} {'Class 1 (Val)':<15} {'Ratio':<10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_val, y_train_val), 1):\n",
    "    y_val_fold = y_train_val.iloc[val_idx]\n",
    "    class_0 = (y_val_fold == 0).sum()\n",
    "    class_1 = (y_val_fold == 1).sum()\n",
    "    ratio = class_0 / class_1 if class_1 > 0 else 0\n",
    "    \n",
    "    print(f\"Fold {fold:<3} {len(train_idx):<15,} {len(val_idx):<15,} {class_0:<7,} ({class_0/len(y_val_fold)*100:5.2f}%)  {class_1:<7,} ({class_1/len(y_val_fold)*100:5.2f}%)  {ratio:.2f}:1\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Dataset Split Summary\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train+Validation: {X_train_val.shape[0]:,} samples ({X_train_val.shape[0]/len(X)*100:.1f}%) - Used for 5-Fold CV\")\n",
    "print(f\"Test set:         {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%) - Held out for final evaluation\")\n",
    "print(f\"Total:            {len(X):,} samples (100%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aced7d3",
   "metadata": {},
   "source": [
    "### 4.3 Target Encoding for `city`\n",
    "\n",
    "**Why Target Encoding?**\n",
    "- `city` has 123 unique values (high cardinality)\n",
    "- One-Hot encoding would create 123 columns (curse of dimensionality)\n",
    "- Target encoding maps each city to its average target value\n",
    "\n",
    "**Preventing Data Leakage:**\n",
    "- Encoder fitted ONLY on training data\n",
    "- Then applied to validation and test sets\n",
    "- Cross-validation built into TargetEncoder (cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Feature 11: city\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize Target Encoder\n",
    "target_encoder = TargetEncoder(\n",
    "    cv=5,                    # 5-fold cross-validation to prevent overfitting\n",
    "    smooth='auto',           # Automatic smoothing for low-frequency cities\n",
    "    target_type='binary'     # Binary classification target\n",
    ")\n",
    "\n",
    "# Step 1: Fit encoder on Train+Validation set\n",
    "print(\"\\nStep 1: Fit encoder on Train+Validation set\")\n",
    "print(f\"   Unique cities: {X_train_val['city'].nunique()}\")\n",
    "print(f\"   Total samples: {len(X_train_val):,}\")\n",
    "\n",
    "target_encoder.fit(X_train_val[['city']], y_train_val)\n",
    "print(\"   Encoder trained successfully\")\n",
    "\n",
    "# Step 2: Transform Train+Validation set\n",
    "print(\"\\nStep 2: Encode Train+Validation set\")\n",
    "X_train_val_encoded = X_train_val.copy()\n",
    "X_train_val_encoded['city_encoded'] = target_encoder.transform(X_train_val[['city']])\n",
    "X_train_val_encoded = X_train_val_encoded.drop('city', axis=1)\n",
    "print(f\"   Train+Val set encoded\")\n",
    "print(f\"   Encoding range: [{X_train_val_encoded['city_encoded'].min():.4f}, {X_train_val_encoded['city_encoded'].max():.4f}]\")\n",
    "\n",
    "# Step 3: Transform Test set\n",
    "print(\"\\nStep 3: Encode Test set (using Train+Val encoding)\")\n",
    "X_test_encoded = X_test.copy()\n",
    "X_test_encoded['city_encoded'] = target_encoder.transform(X_test[['city']])\n",
    "X_test_encoded = X_test_encoded.drop('city', axis=1)\n",
    "print(f\"   Test set encoded\")\n",
    "print(f\"   Test set cities: {X_test['city'].nunique()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Target Encoding Complete\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train+Validation set: {X_train_val_encoded.shape}\")\n",
    "print(f\"Test set:             {X_test_encoded.shape}\")\n",
    "print(f\"\\nAll features are now numeric and ready for 5-Fold Cross-Validation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99dcf20",
   "metadata": {},
   "source": [
    "### 4.4 Final Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96216e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset summary\n",
    "print(\"=\"*80)\n",
    "print(\"Final Dataset Summary\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Dataset Shapes:\")\n",
    "print(f\"   Train+Validation: X_train_val_encoded {X_train_val_encoded.shape}, y_train_val {y_train_val.shape}\")\n",
    "print(f\"   Test set:         X_test_encoded      {X_test_encoded.shape}, y_test      {y_test.shape}\")\n",
    "\n",
    "print(\"\\n2. Number of Features:\")\n",
    "print(f\"   Total features: {X_train_val_encoded.shape[1]}\")\n",
    "\n",
    "print(\"\\n3. Missing Values Check:\")\n",
    "print(f\"   Train+Val missing: {X_train_val_encoded.isnull().sum().sum()}\")\n",
    "print(f\"   Test missing:      {X_test_encoded.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\n4. Feature List (all features):\")\n",
    "feature_cols = X_train_val_encoded.columns.tolist()\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "print(\"\\n5. Sample Data (Train+Val first 3 rows):\")\n",
    "display(X_train_val_encoded.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Dataset Preparation Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Ready for 5-Fold Cross-Validation with {X_train_val_encoded.shape[0]:,} samples\")\n",
    "print(f\"Test set ({X_test_encoded.shape[0]:,} samples) reserved for final evaluation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
